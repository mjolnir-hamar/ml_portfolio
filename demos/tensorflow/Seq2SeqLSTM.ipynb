{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b2c9dd-aee7-4cf1-aa02-abe26705cd0c",
   "metadata": {},
   "source": [
    "# Seq2Seq LSTM (Tensorflow)\n",
    "\n",
    "The basic code on which this is based comes from [this Keras tutorial on a Seq2Seq LSTM model](https://keras.io/examples/nlp/lstm_seq2seq/). However, this differs in a few key places:\n",
    "\n",
    "1. The tutorial model predicts the text at the character level (character-by-character). This model predicts at the token level.\n",
    "2. The tutorial uses basic one-hot-encoding for characters. This model uses word embeddings.\n",
    "3. This code not only includes a demo on how sequence prediction (decoding) works but also how metrics calculation can be done using BLEU score, a standard metric for machine translation tasks.\n",
    "4. The tutorial constructs the inference models after training the base model. This code constructs the training model along with the inference encoder and decoder at the same time, leveraging the fact that the encoder and decoder are simply re-using components of the training model that will be fit to the data during training.\n",
    "\n",
    "The data used can be found [here](http://www.manythings.org/anki/fra-eng.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc40846-469c-470b-9072-91a0cd300a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from statistics import mean\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329a3cb-928b-480d-8c1a-91b707507c7f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d8fa56-40ce-458a-9e3c-120e95872ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d737d-0345-4b7c-8b59-b417f6596c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64        # batch size for training\n",
    "EPOCHS = 125           # number of epochs to train for\n",
    "LATENT_DIM = 256       # latent dimensionality of the encoding space\n",
    "NUM_SAMPLES = 10000    # number of samples to train on\n",
    "EMBED_SIZE = 512       # number or params in the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b314c-df69-4ade-8729-edf91da6fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "DATA_PATH = \"fra.txt\"  # path to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5af055-54fd-41a4-89c1-66a3a34421e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenization utils\n",
    "BOS = \"[BOS]\"          # beginning of sequence token\n",
    "EOS = \"[EOS]\"          # end of sequence token\n",
    "PAD = \"[PAD]\"          # padding token\n",
    "SPEC_TOKENS = [PAD, BOS, EOS]\n",
    "\n",
    "# Punctuation regex\n",
    "PUNKT_RE = re.compile(r\"[\\.!\\?]+\")\n",
    "# Multiple space regex\n",
    "MULTI_SPACE_RE = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "def format_text(text):\n",
    "    '''\n",
    "    Remove punctuation, consolidate white space, lowercase,\n",
    "    and split a sequence\n",
    "    '''\n",
    "    return MULTI_SPACE_RE.sub(\n",
    "        \" \", PUNKT_RE.sub(\n",
    "            \"\", text\n",
    "        )\n",
    "    ).lower().strip().split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b14f59-daa4-4e66-87f4-5fbe6fa782d0",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc779344-e989-419a-bbbf-e7deb23a68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "\n",
    "# Input/target text lists\n",
    "INPUT_TEXTS = []\n",
    "TARGET_TEXTS = []\n",
    "\n",
    "# Input/target token sets\n",
    "INPUT_TOKENS = set()\n",
    "TARGET_TOKENS = set()\n",
    "\n",
    "# Dictionary of input sequence to output sequence(s)\n",
    "SRC_TO_TGT_MAP = defaultdict(set)\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as _f:\n",
    "    for i, line in enumerate(_f):\n",
    "        if i == NUM_SAMPLES:\n",
    "            break\n",
    "\n",
    "        input_text, target_text = line.split(\"\\t\")[:2]\n",
    "        input_text = format_text(input_text)\n",
    "        target_text = format_text(target_text)\n",
    "\n",
    "        SRC_TO_TGT_MAP[\" \".join(input_text)].add(\" \".join(target_text))\n",
    "\n",
    "        target_text = [BOS] + target_text + [EOS]\n",
    "\n",
    "        INPUT_TEXTS.append(input_text)\n",
    "        TARGET_TEXTS.append(target_text)\n",
    "\n",
    "        INPUT_TOKENS |= set(input_text)\n",
    "        TARGET_TOKENS |= set(target_text)\n",
    "\n",
    "# Aggregate tokens, sequences, and counts\n",
    "ALL_TOKENS = sorted(list(INPUT_TOKENS | TARGET_TOKENS))\n",
    "NUM_ALL_TOKENS = len(ALL_TOKENS)\n",
    "INPUT_TOKENS = sorted(list(INPUT_TOKENS))\n",
    "TARGET_TOKENS = sorted(list(TARGET_TOKENS))\n",
    "NUM_INPUT_TOKENS = len(INPUT_TOKENS) + len(SPEC_TOKENS)\n",
    "NUM_TARGET_TOKENS = len(TARGET_TOKENS) + len(SPEC_TOKENS)\n",
    "MAX_INPUT_SEQ_LEN = max([len(txt) for txt in INPUT_TEXTS])\n",
    "MAX_TARGET_SEQ_LEN = max([len(txt) for txt in TARGET_TEXTS])\n",
    "\n",
    "# Token-to-index (and reverse) mappings\n",
    "INPUT_TOK_TO_IDX = {tok: i for i, tok in enumerate(SPEC_TOKENS)}\n",
    "TARGET_TOK_TO_IDX = {tok: i for i, tok in enumerate(SPEC_TOKENS)}\n",
    "for i, tok in enumerate(INPUT_TOKENS):\n",
    "    INPUT_TOK_TO_IDX[tok] = i + len(SPEC_TOKENS)\n",
    "INPUT_IDX_TO_TOK = {i: tok for tok, i in INPUT_TOK_TO_IDX.items()}\n",
    "for i, tok in enumerate(TARGET_TOKENS):\n",
    "    TARGET_TOK_TO_IDX[tok] = i + len(SPEC_TOKENS)\n",
    "TARGET_IDX_TO_TOK = {i: tok for tok, i in TARGET_TOK_TO_IDX.items()}\n",
    "\n",
    "print(f\"NUMBER OF SAMPLES: {len(INPUT_TEXTS)}\")\n",
    "print(f\"NUMBER OF UNIQUE INPUT TOKENS: {NUM_INPUT_TOKENS}\")\n",
    "print(f\"NUMBER OF UNIQUE TARGET TOKENS: {NUM_TARGET_TOKENS}\")\n",
    "print(f\"MAX INPUT SEQ LEN: {MAX_INPUT_SEQ_LEN}\")\n",
    "print(f\"MAX TARGET SEQ LEN: {MAX_TARGET_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5f792-93e0-4627-9ede-495c151d404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up encoder input & decoder input/target data as numpy arrays for training.\n",
    "# This is done manually here but can also be completed using different methods\n",
    "# and classes from Keras like its Tokenizer class and tokens_to_word_sequences.\n",
    "# The purpose is to simply get a space-delimited series of tokens into a vector\n",
    "# where each token is represented by a unique number or vector of numbers (as\n",
    "# in one-hot encoding).\n",
    "\n",
    "# Encoder/decoder input data are only single-dimension arrays per sequence.\n",
    "# Tokens are converted to indexs from the tok_to_idx mappings, only integers.\n",
    "# These will be converted in the embeedding layers\n",
    "ENCODER_INPUT_DATA = np.zeros((len(INPUT_TEXTS), MAX_INPUT_SEQ_LEN), dtype=\"int32\")\n",
    "DECODER_INPUT_DATA = np.zeros((len(INPUT_TEXTS), MAX_TARGET_SEQ_LEN), dtype=\"int32\")\n",
    "\n",
    "# Decoder target data is multi-dimensional arrays per sequence where each token is\n",
    "# one-hot encoded. Target data is handled as a multi-class classification problem\n",
    "# (i.e. which token has the highest logit).\n",
    "DECODER_TARGET_DATA = np.zeros((len(INPUT_TEXTS), MAX_TARGET_SEQ_LEN, NUM_TARGET_TOKENS), dtype=\"float32\")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(INPUT_TEXTS, TARGET_TEXTS)):\n",
    "    for j, in_tok in enumerate(input_text):\n",
    "        ENCODER_INPUT_DATA[i, j] = np.int32(INPUT_TOK_TO_IDX[in_tok])\n",
    "    for k, tgt_tok in enumerate(target_text):\n",
    "        DECODER_INPUT_DATA[i, k] = np.int32(TARGET_TOK_TO_IDX[tgt_tok])\n",
    "        if k > 0:\n",
    "            DECODER_TARGET_DATA[i, k-1, TARGET_TOK_TO_IDX[tgt_tok]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913efb0-40fc-4985-8855-64df46ff23a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb899957-00c5-412e-ac2a-ba7002f6842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models\n",
    "\n",
    "# Shared embedding\n",
    "embed_layer = keras.layers.Embedding(input_dim = NUM_ALL_TOKENS, output_dim = EMBED_SIZE)\n",
    "\n",
    "# Encoder parts\n",
    "encoder_inputs = keras.Input(shape=(MAX_INPUT_SEQ_LEN, ))\n",
    "encoder_embedding = embed_layer(encoder_inputs)\n",
    "encoder_lstm = keras.layers.LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_lstm_outputs, enc_state_h, enc_state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [enc_state_h, enc_state_c]\n",
    "\n",
    "# Decoder parts\n",
    "decoder_inputs = keras.Input(shape=(MAX_TARGET_SEQ_LEN, ))\n",
    "decoder_embedding = embed_layer(decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_lstm_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(NUM_TARGET_TOKENS, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# Training model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Encoder for inference\n",
    "encoder = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder for inference\n",
    "decoder_state_input_h = keras.Input(shape=(LATENT_DIM, ))\n",
    "decoder_state_input_c = keras.Input(shape=(LATENT_DIM, ))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_infer_lstm_outputs, dec_state_h, dec_state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [dec_state_h, dec_state_c]\n",
    "decoder_infer_outputs = decoder_dense(decoder_infer_lstm_outputs)\n",
    "decoder = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_infer_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692d582-a70b-41d0-bc2e-74e8bf104241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show structure of the training model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a83b5c-866f-463a-8816-59379a70ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show structure of the inference encoder\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e68a16-179d-4931-b278-773a73e4a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show structure of the inference decoder\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d84da2-6aed-4508-b0e9-08dc69fc60b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "with tf.device(gpus[0].name):\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(\n",
    "        [ENCODER_INPUT_DATA, DECODER_INPUT_DATA],\n",
    "        DECODER_TARGET_DATA,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1b3b1-3a0a-4ef7-b265-62cb3273a17d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a1c90-a0b4-4af5-b56f-3f0863b33f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    '''\n",
    "    Take an input sequence and get its model prediction.\n",
    "    '''\n",
    "\n",
    "    # Run the input seq through the encoder to get the encoder states.\n",
    "    states_value = encoder.predict(input_seq)\n",
    "\n",
    "    # Construct a single input sequence and insert the BOS token in the first slot.\n",
    "    target_seq = np.zeros((1, MAX_TARGET_SEQ_LEN), dtype=\"int32\")\n",
    "    target_seq[0, 0] = TARGET_TOK_TO_IDX[BOS]\n",
    "\n",
    "    # Iterate until 1 or the 2 stop conditions are met:\n",
    "    #  1. Decoder predicts the EOS token\n",
    "    #  2. Maximum target seq length is hit\n",
    "    stop_condition = False\n",
    "    decoded_sequence = []\n",
    "    while not stop_condition:\n",
    "        # Get the decoder prediction and updated states by passing it the\n",
    "        # target sequence and the encoder states (or updated decoder states\n",
    "        # if this isn't the first pass of the iteration).\n",
    "        output_token, h, c = decoder.predict([target_seq] + states_value)\n",
    "\n",
    "        # Get the token's index as the argmax of decoder output logits.\n",
    "        sampled_token_idx = np.argmax(output_token[0][0])\n",
    "        # Convert index to the actual token.\n",
    "        sampled_token = TARGET_IDX_TO_TOK[sampled_token_idx]\n",
    "\n",
    "        # Check stop conditions.\n",
    "        if sampled_token == EOS or len(decoded_sequence) > MAX_TARGET_SEQ_LEN:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sequence.append(sampled_token)\n",
    "\n",
    "        # Update the target sequence.\n",
    "        target_seq = np.zeros((1, MAX_TARGET_SEQ_LEN), dtype=\"int32\")\n",
    "        target_seq[0, 0] = sampled_token_idx\n",
    "\n",
    "        # Update the states with the updated states from the decoder.\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return \" \".join(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc7aeb-aa5d-4ce1-ae77-cb31ba7d9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(refs, hyp):\n",
    "    '''\n",
    "    Calculate BLEU score as the mean of unigram through 4-gram BLEU scores.\n",
    "    '''\n",
    "    return mean(\n",
    "        nltk.translate.bleu_score.sentence_bleu(\n",
    "            [ref.split(\" \") for ref in refs],\n",
    "            hyp.split(\" \"),\n",
    "            weights=[(1.0, ), (1./2., 1./2.),  (1./3., 1./3., 1./3.),  (1./4., 1./4., 1./4., 1./4.)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8683c5-2add-45ee-bd5d-b9c5eb92fede",
   "metadata": {},
   "source": [
    "### Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca2fb2-84fe-4bcc-a938-dd19ddb01147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1 input sequence for demo\n",
    "input_seq = ENCODER_INPUT_DATA[56:57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729b814-9fa4-4698-906d-2e2d821ef982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the input sequence\n",
    "input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2e32c-5b13-430e-8fc9-0b726e5f917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the input sequence converted back into tokens\n",
    "[INPUT_IDX_TO_TOK[idx] for idx in input_seq[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f777a-1985-4aa4-93ad-fdb33653d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoded prediction\n",
    "print(decode_sequence(input_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ef94d-7e80-4444-9e1c-206337b1d02f",
   "metadata": {},
   "source": [
    "### Full Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cdcde8-b1d4-4231-89de-a4e69e597b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run on 20 unique input sequences\n",
    "TEST_DECODE_MAX = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516329b-bfeb-4316-84ed-53f55c902f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather 1 prediction per input sequence.\n",
    "# Input sequences exist more than once in INPUT_TEXTS because each\n",
    "# has more than 1 translation in the linked data. We only want the top one.\n",
    "preds = {}\n",
    "for i, input_text in enumerate(INPUT_TEXTS):\n",
    "    if len(preds.keys()) == TEST_DECODE_MAX:\n",
    "        break\n",
    "    input_text = \" \".join(input_text)\n",
    "    if input_text not in preds.keys():\n",
    "        if input_text == \"i see\": print(i)\n",
    "        input_seq = ENCODER_INPUT_DATA[i:i+1]\n",
    "\n",
    "        with tf.device(gpus[0].name):\n",
    "            decoded_seq = decode_sequence(input_seq)\n",
    "\n",
    "        preds[input_text] = decoded_seq    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c40dea-1442-491c-802d-fb2ea10577d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results.\n",
    "# For each input text, show:\n",
    "#  1. The text, itself\n",
    "#  2. The model's hypothesis\n",
    "#  3. The possible correct references\n",
    "#  4. If the hypothesis is one of the references\n",
    "#  5. The BLEU score of the hypothesis against the references\n",
    "for src, hyp in preds.items():\n",
    "    refs = SRC_TO_TGT_MAP[src]\n",
    "    print(\"----------------------\")\n",
    "    print(f\"Source: \\\"{src}\\\"\")\n",
    "    print(f\"Hypothesis Translation: \\\"{hyp}\\\"\")\n",
    "    print(f\"Reference Translations: \\\"{refs}\\\"\")\n",
    "    print(f\"Hypothesis in references: {hyp in refs}\")\n",
    "    print(f\"BLEU Score: {calculate_bleu(refs, hyp)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
